% Credits to Ning Yuan for the format

\documentclass[10pt,landscape]{article}
\usepackage{amssymb,amsmath,amsthm,amsfonts,bm}
\newenvironment{amatrix}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}
\usepackage{multicol,multirow}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage[colorlinks=true,citecolor=blue,linkcolor=blue]{hyperref}
\ifthenelse{\lengthtest { \paperwidth = 11in}}
    { \geometry{top=.2in,left=.2in,right=.2in,bottom=.2in} }
	{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
		{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	}
\pagestyle{empty}
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%x
                                {\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
                                {-1explus -.5ex minus -.2ex}%
                                {0.5ex plus .2ex}%
                                {\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
                                {-1ex plus -.5ex minus -.2ex}%
                                {1ex plus .2ex}%
                                {\normalfont\small\bfseries}}
\makeatother
\setcounter{secnumdepth}{0}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}



\theoremstyle{definition}
\newcommand{\thistheoremname}{}
\newtheorem*{genericthm*}{\thistheoremname}
\newenvironment{namedthm*}[1]
{\renewcommand{\thistheoremname}{#1}\begin{genericthm*}}
{\end{genericthm*}}

% -----------------------------------------------------------------------

\title{MA2213 AY19/20 Semester 1 Finals}

\begin{document}

\begin{center}
	{\large MA2213 AY19/20 Semester 1 Finals}
\end{center}

\raggedright
\footnotesize

\begin{multicols}{3}

	\setlength{\premulticols}{1pt}
	\setlength{\postmulticols}{1pt}
	\setlength{\multicolsep}{1pt}
	\setlength{\columnsep}{2pt}

	\section{Lecture 8: Lagrange Interpolation}
	\begin{namedthm*}{Solving via linear system} \label{ge}
		\[\begin{bmatrix}
				1      & x_0    & x_0^2  & \cdots & x_0^n  \\
				1      & x_1    & x_1^2  & \cdots & x_1^n  \\
				\vdots & \vdots & \vdots & \ddots & \vdots \\
				1      & x_n    & x_n^2  & \cdots & x_n^n  \\
			\end{bmatrix}
			\begin{bmatrix}
				a_0    \\
				a_1    \\
				\vdots \\
				a_n
			\end{bmatrix}
			=
			\begin{bmatrix}
				f(x_0) \\
				f(x_1) \\
				\vdots \\
				f(x_n)
			\end{bmatrix}
		\]
	\end{namedthm*}

	\begin{namedthm*}{Solving via basis polynomials} Let
		\(L_{k}(x)=\frac{\left(x-x_{0}\right) \cdots\left(x-x_{k-1}\right)\left(x-x_{k+1}\right) \cdots\left(x-x_{n}\right)}{\left(x_{k}-x_{0}\right) \cdots\left(x_{k}-x_{k-1}\right)\left(x_{k}-x_{k+1}\right) \cdots\left(x_{k}-x_{n}\right)}\) for \(k=0,1, \cdots, n\). Then \(P_{n}(x)=y_{0} L_{0}(x)+y_{1} L_{1}(x)+\cdots+y_{n} L_{n}(x)\).
	\end{namedthm*}

	\begin{namedthm*}{Error analysis}
		\(\forall x \in[a, b], \exists \xi \in\left(\min \left\{x, x_{0}, x_{1}, \cdots, x_{n}\right\}, \max \left\{x, x_{0}, x_{1}, \cdots, x_{n}\right\}\right)\) such that
		\[
			f(x)=P_{n}(x)+\frac{f^{(n+1)}(\xi)}{(n+1) !}\left(x-x_{0}\right)\left(x-x_{1}\right) \cdots\left(x-x_{n}\right)
		\]
	\end{namedthm*}
	\section{Tutorial 3: Lagrange Interpolation}
	\begin{namedthm*}{Example 1} Construct the LIP for \(f(x)=\log _{2} x\) using \(x_{0}=1 / 2, x_{1}=1, x_{2}=2, x_{3}=4 .\) Find a bound of the absolute error for any
		\(x \in[1 / 2,+\infty) .\)
		\\\textbf{Solution.} \(f\left(0.5\right)=-1, \quad f\left(1\right)=0, \quad f\left(2\right)=1, \quad f\left(4\right)=2\). Solve using linear system
		to get \(a_0= -\frac{52}{21}\), \(a_1 = \frac{7}{2}\), \(a_2 =-\frac{7}{6}\), \(a_3=\frac{1}{7}\). So \(P(x)=\frac{1}{7} x^{3}-\frac{7}{6} x^{2}+\frac{7}{2} x-\frac{52}{21}\). To get an u.b for error, first find \(f^{(4)}(x)=-\frac{6}{x^{4} \ln 2}\). Note monotonicity. Hence \(\left|f^{(4)}(x)\right| \leqslant \frac{6}{(1 / 2)^{4} \ln 2}=\frac{96}{\ln 2}\). Thus an u.b for absolute error \(|P(x)-f(x)|\) is
		\(
		\frac{1}{4 !} \times \frac{96}{\ln 2}|(x-1 / 2)(x-1)(x-2)(x-4)|=\frac{4}{\ln 2}|(x-1 / 2)(x-1)(x-2)(x-4)|. \medspace \square
		\).
	\end{namedthm*}

	\begin{namedthm*}{Result from Example 2}
		If nodes are equidistributed, the maximum value of \(g(x)=\left|\left(x-x_{0}\right)\left(x-x_{1}\right) \cdots\left(x-x_{N}\right)\right|\) must be attained in \(\left(x_{0}, x_{1}\right)\) and \(\left(x_{N-1}, x_{N}\right)\) (due to the symmetry). \(\left|g\left(x^{*}\right)\right| \leqslant \frac{1}{4} N ! h^{N+1}\).
	\end{namedthm*}
	\begin{namedthm*}{Error estimation for equidistributed nodes}
		\(\left|P_{N}(x)-f(x)\right| \leqslant \frac{h^{N+1}}{4(N+1)} \max_{\xi \in[a, b]}\left|f^{(N+1)}(\xi)\right|,\) for all \(x \in[a, b]\)
	\end{namedthm*}

	\begin{namedthm*}{Exercise 2} Let \(P_{n}(x)\) be the LIP for \(f(x)=\cos x\) with \(x_{k}=k h, \quad k=0,1,\cdots, n\) where \(h=\pi /(2 n)\). \textbf{1.} Find a positive integer \(N\) such that
		\(
		\left|P_{N}(x)-f(x)\right|<0.005, \quad \text { for all } x \in[0, \pi / 2].
		\)
		\textbf{Solution.} For \(f(x)=\cos(x)\), \(\max _{\xi \in[0, \pi / 2]}\left|f^{(N+1)}(\xi)\right|=1\). Hence it suffices to find \(\frac{h^{N+1}}{4(N+1)} < 0.005 \implies N \geq 3\).
	\end{namedthm*}
	\section{Lecture 9: Divided Differences}
	\begin{namedthm*}{How to find the Lagrange polynomial}
		\(
		P_{n}(x)=a_{0}+a_{1}\left(x-x_{0}\right)+a_{2}\left(x-x_{0}\right)\left(x-x_{1}\right)+\cdots+a_{n}\left(x-x_{0}\right)\left(x-x_{1}\right) \cdots\left(x-x_{n-1}\right)
		\)
		where \(a_{k}=f\left[x_{0}, x_{1}, \cdots, x_{k}\right]\) and \(f\left[x_{0}, x_{1}, \cdots, x_{n}\right]=\frac{f\left[x_{1}, x_{2}, \cdots, x_{n}\right]-f\left[x_{0}, x_{1}, \cdots, x_{n-1}\right]}{x_{n}-x_{0}}\). \(a_0 = f(x_0)\).
	\end{namedthm*}
	\section{Lecture 10: Cubic Spline Interpolation (CSI)}

	\begin{namedthm*}{How to find \(\mu_k \text{ and } \lambda_k\)}\(\mu_{k}=\frac{x_{k}-x_{k-1}}{x_{k+1}-x_{k-1}}, \quad \lambda_{k}=\frac{x_{k+1}-x_{k}}{x_{k+1}-x_{k-1}}, \quad k=1,2, \cdots, n-1\)
	\end{namedthm*}

	\begin{namedthm*}{Natural Boundary Conditions}\(M_0 = M_n = 0\).
		\[
			\begin{bmatrix}
				2     & \lambda_1 &           &           &               \\
				\mu_2 & 2         & \lambda_2 &           &               \\
				      & \mu_3     & 2         & \ddots    &               \\
				      &           & \ddots    & \ddots    & \lambda_{n-2} \\
				      &           &           & \mu_{n-1} & 2
			\end{bmatrix}
			\begin{bmatrix}
				M_1     \\
				M_2     \\
				M_3     \\
				\vdots  \\
				M_{n-2} \\
				M_{n-1} \\
			\end{bmatrix} =
			\begin{bmatrix}
				6f[x_0,x_1,x_2]             \\
				6f[x_1,x_2,x_3]             \\
				6f[x_2,x_3,x_4]             \\
				\vdots                      \\
				6f[x_{n-3},x_{n-2},x_{n-1}] \\
				6f[x_{n-2},x_{n-1}, x_n]    \\
			\end{bmatrix}
		\]
	\end{namedthm*}

	\begin{namedthm*}{Clamped Boundary Conditions}\(2 M_{0}+M_{1}=6 f\left[x_{0}, x_{0}, x_{1}\right], \quad M_{n-1}+2 M_{n}=6 f\left[x_{n-1}, x_{n}, x_{n}\right]\).
		\[
			\begin{bmatrix}
				2     & \lambda_0 &           &        &               \\
				\mu_1 & 2         & \lambda_1 &        &               \\
				      & \ddots    & \ddots    & \ddots &               \\
				      &           & \mu_{n-1} & 2      & \lambda_{n-1} \\
				      &           &           & \mu_n  & 2
			\end{bmatrix}
			\begin{bmatrix}
				M_0     \\
				M_1     \\
				\vdots  \\
				M_{n-1} \\
				M_{n}
			\end{bmatrix} =
			\begin{bmatrix}
				6f[x_0,x_0,x_1]           \\
				6f[x_0,x_1,x_2]           \\
				\vdots                    \\
				6f[x_{n-2},x_{n-1},x_{n}] \\
				6f[x_{n-1},x_{n}, x_n]
			\end{bmatrix}
		\]
	\end{namedthm*}

	\begin{namedthm*}{How to find \(S_k\)} \(S_{k}(x)=M_{k-1} \frac{\left(x-x_{k}\right)^{3}}{6\left(x_{k-1}-x_{k}\right)}+M_{k} \frac{\left(x-x_{k-1}\right)^{3}}{6\left(x_{k}-x_{k-1}\right)}+A_{k} x+B_{k}\). \(A_{k}=\frac{f\left(x_{k}\right)-f\left(x_{k-1}\right)}{x_{k}-x_{k-1}}-\frac{1}{6}\left(M_{k}-M_{k-1}\right)\left(x_{k}-x_{k-1}\right)\).\\ \(B_{k}=\frac{x_{k} f\left(x_{k-1}\right)-x_{k-1} f\left(x_{k}\right)}{x_{k}-x_{k-1}}+\frac{1}{6}\left(M_{k} x_{k-1}-M_{k-1} x_{k}\right)\left(x_{k}-x_{k-1}\right)\).
	\end{namedthm*}
	\begin{namedthm*}{Piecewise Linear Interpolation (PLI)} If \(S(x) = S_k(x), \quad \text {for } x\in [x_{k-1}, x_k], k = 1,2,\cdots, n\) then
		\(S_k(x)=f(x_{k-1}) \frac{x-x_k}{x_{k-1}-x_{k}}+f(x_k) \frac{x-x_{k-1}}{x_k-x_{k-1}}\)
	\end{namedthm*}
	\begin{namedthm*}{Error analysis for PLI on equidistributed nodes}
		If \(x_{k}=x_{0}+k h\), then for \(x \in [x_0,x_n]\), \(|f(x)-S(x)| \leq \frac{1}{8}h^2\max_{\xi \in [x_0,x_n]}|f''(\xi)|\)
	\end{namedthm*}


	\section{Tutorial 4: Divided Diff and CSI}
	\begin{namedthm*}{Example 2: Quadratic spline interpolation}
		Given \(n+1\) nodes \(x_{0}<x_{1}<\cdots<x_{n-1}<x_{n}\) and a continuous function \(f(x),\)
		find a function \(S(x)\) such that
		\textbf{1.} \(S(x)\) is first-order differentiable on \(\left(x_{0}, x_{n}\right)\)
		\textbf{2.} \(S(x)\) is a quadratic polynomial on \(\left(x_{k-1}, x_{k}\right)\) for any \(k=2,3, \cdots, n ;\)
		\textbf{3.} \(\left.S(x) \text { is a linear function on (} x_{0}, x_{1}\right)\). \textbf{4.} \(S\left(x_{k}\right)=f\left(x_{k}\right)\) for all \(k=0,1, \cdots, n\).
		\\\textbf{Solution.} \(S_{1}(x)=f\left(x_{0}\right) \frac{x-x_{1}}{x_{0}-x_{1}}+f\left(x_{1}\right) \frac{x-x_{0}}{x_{1}-x_{0}}\). \(S_{k}(x)=\frac{1}{2} M_{k} \frac{\left(x-x_{k-1}\right)^{2}}{x_{k}-x_{k-1}}+\frac{1}{2} M_{k-1} \frac{\left(x-x_{k}\right)^{2}}{x_{k-1}-x_{k}}+C_{k}, \quad k=2,3, \cdots, n.\)\\\(C_{k}=f\left(x_{k}\right)-\frac{1}{2} M_{k}\left(x_{k}-x_{k-1}\right)\). \(M_{k}=2 f\left[x_{k-1}, x_{k}\right]-M_{k-1}\). Since \(M_{1}=S_{1}^{\prime}\left(x_{1}\right)=f\left[x_{0}, x_{1}\right]\), \(M_{k}\) for \(k=2,3, \cdots, n\) can be obtained iteratively.
	\end{namedthm*}
	\section{Lecture 11: Least Squares Approximation}
	\begin{namedthm*}{Proof of optimality}
		Suppose a satisfies \(
		X^{T} X \mathbf{a}=X^{T} \mathbf{y}.\) Then for any vector b with the same length as a, we have
		\(
		(X \mathbf{b}-\mathbf{y})^{T}(X \mathbf{b}-\mathbf{y}) \geqslant(X \mathbf{a}-\mathbf{y})^{T}(X \mathbf{a}-\mathbf{y})\). \textbf{Proof.}\[
			\begin{aligned}&(X \mathbf{b}-\mathbf{y})^{T}(X \mathbf{b}-\mathbf{y})-(X \mathbf{a}-\mathbf{y})^{T}(X \mathbf{a}-\mathbf{y}) \\=& \mathbf{b}^{T} X^{T} X \mathbf{b}-2 \mathbf{b}^{T} X^{T} \mathbf{y}-\mathbf{a}^{T} X^{T} X \mathbf{a}+2 \mathbf{a}^{T} X^{T} \mathbf{y} \\=& \mathbf{b}^{T} X^{T} X \mathbf{b}-2 \mathbf{b}^{T} X^{T} X \mathbf{a}-\mathbf{a}^{T} X^{T} X \mathbf{a}+2 \mathbf{a}^{T} X^{T} X \mathbf{a} \\=& \mathbf{b}^{T} X^{T} X \mathbf{b}-2 \mathbf{b}^{T} X^{T} X \mathbf{a}+\mathbf{a}^{T} X^{T} X \mathbf{a}=(\mathbf{b}-\mathbf{a})^{T} X^{T} X(\mathbf{b}-\mathbf{a}) \geqslant 0 \end{aligned}
		\]

	\end{namedthm*}
	\begin{namedthm*}{Finding the coefficients}
		Let \(X=\left(\begin{array}{ccccc}{1} & {x_{0}} & {x_{0}^{2}} & {\cdots} & {x_{0}^{n}} \\ {1} & {x_{1}} & {x_{1}^{2}} & {\cdots} & {x_{1}^{n}} \\ {\vdots} & {\vdots} & {\vdots} & {} & {\vdots} \\ {1} & {x_{m}} & {x_{m}^{2}} & {\cdots} & {x_{m}^{n}}\end{array}\right)\) and
		\\ \(\mathbf{a}=\left(a_{0}, a_{1}, \cdots, a_{n}\right)^{T}, \quad \mathbf{y}=\left(y_{0}, y_{1}, \cdots, y_{m}\right)^{T}\). Solve \(X^TXa = X^Ty.\)
	\end{namedthm*}

	\begin{namedthm*}{Lecture Exercise 1} Show that the least squares approximation is unique if and only if the matrix \(X\)
		has full column rank, \(i.e.,\) the rank of \(X\) equals its number of columns. \textbf{Proof.} Note \(\operatorname{rank}(X^TX)=\operatorname{rank}(X)\). \((\implies)\)If the LSA is unique, the columns are linearly independent since we can write \(Xa\) as a unique linear combination of the columns of \(X\). Hence the rank of the matrix is equal to the number of columns.\((\impliedby)\) If \(Xa = b \text{ and } Xa' = b\) then \(X(a-a')=0\). Since X has full rank, \(Xc = 0\) iff \(c = 0 \implies a = a'\) (only the trivial solution to the homogeneous equation of linear combination of its columns exists).
	\end{namedthm*}
	\begin{namedthm*}{Weighted LSA}
		\(W=\operatorname{diag}\left\{w_{0}, w_{1}, \cdots, w_{n}\right\}\). Solve \(X^{T} W X \mathbf{a}=X^{T} W \mathbf{y}\).
	\end{namedthm*}
	\section{Lecture 12: Newton-Cotes Formulae (NCF)}
	\begin{namedthm*}{Trapezoidal Rule}\(\int_{a}^{b} f(x) \mathrm{d} x \approx \frac{b-a}{2}[f(a)+f(b)]\).
	\end{namedthm*}
	\begin{namedthm*}{Error for Trapezoidal Rule}\(\int_{a}^{b} f(x) \mathrm{d} x=\frac{b-a}{2}[f(a)+f(b)]-\frac{1}{12}(b-a)^{3} f^{\prime \prime}(\xi)\)
	\end{namedthm*}
	\begin{namedthm*}{Simpson's Rule} Given \(f(a), f\left(\frac{a+b}{2}\right)\) and \(f(b)\), \(P(x)=f(a)+\frac{f(b)-f(a)}{b-a}(x-a)+\left[2 f\left(\frac{a+b}{2}\right)-f(b)-f(a)\right] \frac{2(x-a)(x-b)}{(b-a)^{2}}\) whose integral is \(\left[\frac{2}{3} f\left(\frac{a+b}{2}\right)+\frac{1}{6} f(b)+\frac{1}{6} f(a)\right](b-a)\).
	\end{namedthm*}
	\begin{namedthm*}{Error for Simpson's Rule}\(\int_{a}^{b} f(x) \mathrm{d} x=\frac{b-a}{6}\left[f(a)+4 f\left(\frac{a+b}{2}\right)+f(b)\right]-\frac{1}{90}\left(\frac{b-a}{2}\right)^{5} f^{(4)}(\xi)\)
	\end{namedthm*}
	\begin{namedthm*}{Theorem 3 on Page 4}
		For closed Newton-Cotes formula with \(n+1\) nodes, when \(n\) is odd and \(f(x)\) is
		\((n+1)\) -th order differentiable, there exists \(\xi \in(a, b)\) such that \(\int_{a}^{b} f(x) \mathrm{d} x=\sum_{k=0}^{n} w_{k} f\left(x_{k}\right)+\frac{h^{n+2} f^{(n+1)}(\xi)}{(n+1) !} \int_{0}^{n} s(s-1) \cdots(s-n) \mathrm{d} s;\) when \(n\) is even and \(f(x)\) is \((n+2)\)-th order differentiable, there exists \(\xi \in(a, b)\) such that \(\int_{a}^{b} f(x) \mathrm{d} x=\sum_{k=0}^{n} w_{k} f\left(x_{k}\right)+\frac{h^{n+3} f^{(n+2)}(\xi)}{(n+2) !} \int_{0}^{n} s^{2}(s-1) \cdots(s-n) \mathrm{d} s\).
	\end{namedthm*}
	\begin{namedthm*}{Gaussian Elimination to find weights}
		\(\int_{a}^{b} x^{j} d x=\sum_{k=0}^{n} w_{k} x_{k}^{j}, \quad j=0,1, \cdots, n\)\\\(\left[\begin{array}{ccccc}{1} & {1} & {\cdots} & {1} \\ {x_{0}} & {x_{1}} & {\cdots}  & {x_{n}} \\ {\vdots} & {\vdots} & {} & {\vdots} \\ {x_{0}^{n-1}} & {x_{1}^{n-1}} & {\cdots} & {x_{n}^{n-1}} \\ {x_{0}^{n}} & {x_{1}^{n}} & {\cdots} & {x_{n}^{n}}\end{array}\right]\left[\begin{array}{c}{w_{0}} \\ {w_{1}} \\ {\vdots} \\ {w_{n-1}} \\ {w_{n}}\end{array}\right]=\left[\begin{array}{c}{\int_{a}^{b} d x} \\ {\int_{a}^{b} x d x} \\ {\vdots} \\ {\int_{a}^{b} x^{n-1} d x} \\ {c^{b} x^{n} d x}\end{array}\right]\)
	\end{namedthm*}
	\begin{namedthm*}{How to find degree of accuracy}
		If \(\int_a^bx^jdx = \sum_{k=0}^nw_kx_k^j\) for \(j = 0,\cdots,n\) but \(\int_a^bx^{n+1}dx \neq \sum_{k=0}^nw_kx_k^{n+1}\) then \(n\) is the degree of accuracy.
	\end{namedthm*}
	\begin{namedthm*}{General form of NCF}
		\(\int_{a}^{b} f(x) \mathrm{d} x \approx \int_{a}^{b} P(x) \mathrm{d} x=\sum_{k=0}^{n} f\left(x_{k}\right) \int_{a}^{b} L_{k}(x) \mathrm{d} x=\sum_{k=0}^{n} w_{k} f\left(x_{k}\right)\) where \(w_{k}=\int_{a}^{b} L_{k}(x) \mathrm{d} x\)
	\end{namedthm*}
	\begin{namedthm*}{Result from Exercise 2}
		\(w_k=\frac{b-a}{n}\int_0^n\prod_{j=0,j\neq k}^{n}\frac{x-j}{k-j}dx, \quad \forall \text{ } k = 0,\cdots,n.\)
	\end{namedthm*}
	\section{Lecture 13: Composite Numerical Integration}

	\begin{namedthm*}{Composite Trapezoidal Rule (CTR)} Assume \(x_{k}-x_{k-1}=h\) for all \(k=1,2, \cdots, n\). Then
		\(\int_{a}^{b} S(x) \mathrm{d} x=\sum_{k=1}^{n} \frac{h}{2}\left[f\left(x_{k-1}\right)+f\left(x_{k}\right)\right]=h\left[\frac{1}{2} f\left(x_{0}\right)+\sum_{k=1}^{n-1} f\left(x_{k}\right)+\frac{1}{2} f\left(x_{n}\right)\right]\)
	\end{namedthm*}

	\begin{namedthm*}{Error analysis for CTR}
		Suppose \(f(x)\) is second-order continuously differentiable on \([a, b],\) and
		\(
		h=\frac{b-a}{\infty}, \quad x_{k}=k h, \quad k=0,1, \cdots, n
		\). There exists \(\xi \in(a, b)\) such that \(\int_{a}^{b} f(x) \mathrm{d} x=h\left[\frac{1}{2} f\left(x_{0}\right)+\sum_{k=1}^{n-1} f\left(x_{k}\right)+\frac{1}{2} f\left(x_{n}\right)\right]-\frac{b-a}{12} h^{2} f^{\prime \prime}(\xi)\).
	\end{namedthm*}
	\begin{namedthm*}{Composite Simpson's Rule (CSR)}
		\(\begin{aligned} \int_{a}^{b} f(x) \mathrm{d} x & \approx \sum_{k=1}^{n / 2} \frac{h}{3}\left[f\left(x_{2 k-2}\right)+4 f\left(x_{2 k-1}\right)+f\left(x_{2 k}\right)\right] \\ &=\frac{h}{3}\left(f(a)+f(b)+2 \sum_{k=1}^{n / 2-1} f\left(x_{2 k}\right)+4 \sum_{k=1}^{n / 2} f\left(x_{2 k-1}\right)\right) \end{aligned}\)
	\end{namedthm*}
	\begin{namedthm*}{Error analysis for CSR}
		Suppose \(f(x)\) is fourth-order continuously differentiable on \([a, b],\) and
		\[
			h=\frac{b-a}{n}, \quad x_{k}=k h, \quad k=0,1, \cdots, n
		\]When \(n\) is an even integer, there exists \(\xi \in(a, b)\) such that
		\(
		\int_{a}^{b} f(x) \mathrm{d} x=\frac{h}{3}\left(f(a)+f(b)+2 \sum_{k=1}^{n / 2-1} f\left(x_{2 k}\right)+4 \sum_{k=1}^{n / 2} f\left(x_{2 k-1}\right)\right)-\frac{b-a}{180} h^{4} f^{(4)}(\xi)
		\)
	\end{namedthm*}
	\begin{namedthm*}{Achieve abs err \(< \epsilon\)}
		Approximate \(\int_0^2\sqrt{1+x^2}dx\) such that the absolute error is less than \(10^{-3}\).
		For composite trapezoidal rule, \(b=2,a=0\), abs.err \(\leq \frac{1}{6}(\frac{2}{n})^2\max_{\xi \in [0,2]}|f''(\xi)| = \frac{1}{6}(\frac{2}{n})^2\). Hence \(n\geq 26 \implies n= 26.\) For composite Simpson's rule,  abs.err \(\leq \frac{1}{90}(\frac{2}{n})^4\max_{\xi \in [0,2]}|f^{(4)}(\xi)| = \frac{8}{15n^4}\). Hence \(n\geq 4.8 \implies n=6\) because \textbf{we need n to be even.}
	\end{namedthm*}
	\section{Tutorial 5: LSA and Integration}
	\begin{namedthm*}{Q4}
		Consider the following numerical integration: \(\int_{-1}^{1} f(x) \mathrm{d} x \approx w_{1} f\left(x_{1}\right)+w_{2} f\left(x_{2}\right)\).
		How to choose \(x_{1}, x_{2}\) and \(w_{1}, w_{2}\) to achieve maximum degree of accuracy? \textbf{Solution.} For 4 unknowns, set 4 equations: set \(f(x)\) to be \(1, x, x^{2}, x^{3}\) and assume that the numerical integration is exact. Then we get
		equations 1.
		\(
		w_{1}+w_{2}=\int_{-1}^{1} 1 \mathrm{d} x=2
		\), 2.
		\(w_{1} x_{1}+w_{2} x_{2}=\int_{-1}^{1} x \mathrm{d} x=0\), 3.
		\(w_{1} x_{1}^{2}+w_{2} x_{2}^{2}=\int_{-1}^{1} x^{2} \mathrm{d} x=\frac{2}{3}\), 4.
		\(w_{1} x_{1}^{3}+w_{2} x_{2}^{3}=\int_{-1}^{1} x^{3} \mathrm{d} x=0\). Solve to get \(w_1 = w_2 = 1, x_1 = -x_2 = \frac{1}{\sqrt{3}}\). Degree of accuracy is found to be 3.

	\end{namedthm*}

	\section{Miscellaneous}
	\begin{namedthm*}{Figuring out the data points used in LSA} If the \(p(x)\) is given but the data points are incomplete, form the \(X^TXa=X^Ty\) system and select rows that look very similar to eliminate as many variables at once as possible.
	\end{namedthm*}
	\begin{namedthm*}{The Gamma function} \(\Gamma(x)=\int_{0}^{+\infty} t^{x-1} \mathrm{e}^{-t} \mathrm{d} t\) and \(\Gamma(n)=(n-1) !\) for any positive integer \(n\).
	\end{namedthm*}
	\begin{namedthm*}{Linear Algebra and Calculus}
		\(\frac{d}{dt}\int_{a(t)}^{b(t)}q(x)dx= q(b(t))b'(t)-q(a(t))a'(t)\).
		\(\operatorname{rank}(A B) \leqslant \operatorname{rank} A=\operatorname{rank} A^{T}=\operatorname{rank}\left(A^{T} A\right)\)
	\end{namedthm*}
	\begin{namedthm*}{Result from T3 Eg. 2}
		For \(f(x)=1 / \sqrt{x}\),
		\(
		f^{(N+1)}(x)=\left(-\frac{1}{2}\right)^{N+1}(2 N+1) !! x^{-N-3 / 2}
		\).
	\end{namedthm*}
	\begin{namedthm*}{Error proofs from lectures/tutorials}
		\begin{namedthm*}{Proving Lagrange error} Define \(g(t)=f(t)-P_{n}(t)-\left[f(x)-P_{n}(x)\right] \frac{\left(t-x_{0}\right)\left(t-x_{1}\right) \cdots\left(t-x_{n}\right)}{\left(x-x_{0}\right)\left(x-x_{1}\right) \cdots\left(x-x_{n}\right)}\). \(g(x_k)=0\) so by repeated Rolle's theorem, there exists a t such that \(g^{(n+1)}(t)=f^{(n+1)}(t)-\frac{(n+1) !\left[f(x)-P_{n}(x)\right]}{\left(x-x_{0}\right)\left(x-x_{1}\right) \cdots\left(x-x_{n}\right)}= 0\). Make \(f(x)-P_n(x)\) the subject.

		\end{namedthm*}
		\begin{namedthm*}{Proving Trapezoidal Rule error}
			Let \(\zeta=(a+b) / 2\) and \(h=(b-a) / 2 .\) Define
			\[
				g(t)=\int_{\zeta-t}^{\zeta+t} f(x) \mathrm{d} x-t[f(\zeta+t)+f(\zeta-t)], \quad r(t)=g(t)-\left(\frac{t}{h}\right)^{3} g(h).
			\] Idea is to approximate h with t. Then \(g^{\prime}(t)=-t\left[f^{\prime}(\zeta+t)-f^{\prime}(\zeta-t)\right]\), \(r^{\prime}(t)=g^{\prime}(t)-\frac{3 t^{2}}{h^{3}} g(h)\) and \(g(0)=r(0)=r(h)=0\). Clearly \(r(t)\) is differentiable. By Rolle's theorem, we can find \(t_{0} \in(0, h)\) such that \(r^{\prime}\left(t_{0}\right)=0\),
			\(\begin{array}{ll}{\text { i.e. }} & {\frac{3 t_{0}^{2}}{h^{3}} g(h)=g^{\prime}\left(t_{0}\right)=-t_{0}\left[f^{\prime}\left(\zeta+t_{0}\right)-f^{\prime}\left(\zeta-t_{0}\right)\right]}\end{array}\). By the mean value theorem, we can find \(\xi \in\left(\zeta-t_{0}, \zeta+t_{0}\right)\) such that
			\(			f^{\prime}\left(\zeta+t_{0}\right)-f^{\prime}\left(\zeta-t_{0}\right)=2 t_{0} f^{\prime \prime}(\xi)
			\). Rearrange.
		\end{namedthm*}

		\begin{namedthm*}{Estimating the cubic spline interpolation error}
			\(\max _{x \in\left[x_{i-1}, x_{i}\right]}\left|f(x)-p_{3}(x)\right| \leq \frac{(h)^{4}}{4 !} \max _{x \in[a, b]}\left|f^{(4)}(x)\right|\). h refers to the max distance between any two points. This estimation comes from the Lagrange error.
		\end{namedthm*}

		\begin{namedthm*}{Tut 4 Eg. 1}
			Suppose \(f(x)\) is nth order differentiable and \(x_{0}, x_{1}, \cdots, x_{n}\) are \(n+1\) distinct real
			numbers. Show that there exists \(\xi\) between min \(\left\{x_{0}, x_{1}, \cdots, x_{n}\right\}\) and max \(\left\{x_{0}, x_{1}, \cdots, x_{n}\right\}\) such
			that
			\(
			f\left[x_{0}, x_{1}, \cdots, x_{n}\right]=\frac{f^{(n)}(\xi)}{n !}.
			\). \textbf{Solution.} Since the divided difference is independent of the order of its parameters, we can
			assume that \(x_{0}<x_{1}<\cdots<x_{n} .\) Let \(P_{n}(x)\) be the Lagrange interpolating polynomial of \(f(x)\)
			with nodes \(x_{0}, x_{1}, \cdots, x_{n} .\) Then \(\begin{aligned} P_{n}(x)=& f\left[x_{0}\right]+f\left[x_{0}, x_{1}\right]\left(x-x_{0}\right)+f\left[x_{0}, x_{1}, x_{2}\right]\left(x-x_{0}\right)\left(x-x_{1}\right) \\ &+\cdots+f\left[x_{0}, x_{1}, x_{2}, \cdots, x_{n}\right]\left(x-x_{0}\right)\left(x-x_{1}\right) \cdots\left(x-x_{n-1}\right) \end{aligned}\). The \(n\) th derivative of \(P_{n}(x)\) is
			\(
			P_{n}^{(n)}(x)=n ! f\left[x_{0}, x_{1}, x_{2}, \cdots, x_{n}\right]
			\). Now we define
			\(
			g(x)=f(x)-P_{n}(x).
			\) By the definition of interpolation, we know that
			\(
			g\left(x_{0}\right)=g\left(x_{1}\right)=\cdots=g\left(x_{n}\right)=0
			\). By Rolle's theorem, there exists \(\xi \in\left(x_{0}, x_{n}\right)\) such that \(g^{(n)}(\xi)=0 .\) since
			\(
			g^{(n)}(\xi)=f^{(n)}(\xi)-P_{n}^{(n)}(\xi)=f^{(n)}(\xi)-n ! f\left[x_{0}, x_{1}, \cdots, x_{n}\right]
			\)
			we prove the claim by equating the above equation to zero.
		\end{namedthm*}
		\begin{namedthm*}{Tut 5 Eg. 1}
			Show that when \(f\) is second-order differentiable on \([a, b],\) there exists \(\xi \in(a, b)\)
			such that
			\(
			\int_{a}^{b} f(x) \mathrm{d} x-(b-a) f\left(\frac{a+b}{2}\right)=\frac{(b-a)^{3}}{24} f^{\prime \prime}(\xi).
			\) \textbf{Solution.} Let \(\zeta=(a+b) / 2\) and define
			\(
			g(t)=\int_{\zeta-t}^{\zeta+t} f(x) \mathrm{d} x-2 t f(\zeta), \quad h(t)=g(t)-\left(\frac{2 t}{b-a}\right)^{3} g\left(\frac{b-a}{2}\right)
			\). It is obvious that
			\(
			h(0)=h\left(\frac{b-a}{2}\right)=0
			\). By Rolle's theorem, we can find \(t_{0}>0\) such that \(h^{\prime}\left(t_{0}\right)=0,\) i.e.,
			\(
			g^{\prime}\left(t_{0}\right)=3 t_{0}^{2}\left(\frac{2}{b-a}\right)^{3} g\left(\frac{b-a}{2}\right)
			\). Note that
			\(
			g^{\prime}\left(t_{0}\right)=f\left(\zeta+t_{0}\right)+f\left(\zeta-t_{0}\right)-2 f(\zeta)=2 t_{0}^{2} f\left[\zeta-t_{0}, \zeta, \zeta+t_{0}\right]
			\). Therefore we can find \(\xi \in\left(\zeta-t_{0}, \zeta+t_{0}\right) \subset(a, b)\) such that
			\(
			f\left[\zeta-t_{0}, \zeta, \zeta+t_{0}\right]=\frac{1}{2} f^{\prime \prime}(\xi)
			\). Do some substitutions.
		\end{namedthm*}
	\end{namedthm*}
	\begin{namedthm*}{Finding the set of x and y values for the lowest error LIP}
		First find the lowest-error polynomial (LEP), which is \(p(x)=a_{0}+a_{1} x+a_{2} x^{2}+\cdots+a_{m} x^{m}\). From Homework Q3, \(M=\left(\begin{array}{cccc}{b-a} & {\frac{b^{2}-a^{2}}{2}} & {\cdots} & {\frac{b^{m+1}-a^{m+1}}{m+1}} \\ {\frac{b^{2}-a^{2}}{2}} & {\frac{b^{3}-a^{3}}{3}} & {\cdots} & {\frac{b^{m+2}-a^{m+2}}{m+2}} \\ {\vdots} & {\vdots} & {} & {\vdots} \\ {\frac{b^{m+1}-a^{m+1}}{m+1}} & {\frac{b^{m+2}-a^{m+2}}{m+2}} & {\cdots} & {\frac{b^{2 m+1}-a^{2 m+1}}{2 m+1}}\end{array}\right)\) and b = \(\left(\int_{a}^{b} x^{0} f(x) d x, \cdots, \int_{a}^{b} x^{m} f(x) d x\right)^{T}\). Solve \(Ma = b\) to get the coefficients of the LEP. Then reverse-engineer the Gaussian Elimination process for finding \(a\) when constructing the LIP to get x and y (possibly non-unique).
	\end{namedthm*}
	\begin{namedthm*}{Determinant of Vandermonde matrix}
		\(\operatorname{det} X=\prod_{i=1}^{n-1} \prod_{j=i+1}^{n}\left(x_{j}-x_{i}\right)\)
	\end{namedthm*}
\end{multicols}
\end{document}